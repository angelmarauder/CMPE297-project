"""
input_extractor.py

Simple input extraction module that:
- Reads user input from the console
- Sends it to OpenAI to be parsed into a structured JSON format
- Prints the resulting JSON to stdout (ready for a fact-checking layer)

Usage:
    - Set environment variable OPENAI_API_KEY with your OpenAI API key.
    - Run: python input_extractor.py
"""

import os
import json
import re
import sys

try:
        import openai
except ImportError:
        raise ImportError("Install openai package: pip install openai")

from dotenv import load_dotenv

MODEL_NAME = "gpt-4o-mini"
ENV_PATH = r"C:\Study\CMPE297-Cloned\Homework\Week4\w4.env"

# Threshold: Sources and fact-checking with confidence ranking - citation...
# Did AWS down yesterday according to CNN?

# One Clearly defined claims. Return one if many.
# Primary claims - Support Claims -> Sam will handle and pass to Akshay
# Opinions -> Subject to the invididual's perspective - Danny returns
# Pass To Sam Only when Claims is first-passed 

# Expected User's Input/Prompt:
# seems - likely - probably - possibly - may - might - could - unsure - uncertain - doubt - believe - think - feel - suspect - estimate - guess - assume - speculate
# consider - imagine - wonder - hope - wish - prefer - desire - want - intend - plan - aim - strive - seek

# My job: 
# Design the capability to extract the claims from the user's input
# and structure them into a predefined JSON format for further processing.

SCHEMA_INSTRUCTIONS = """
You are ClaimExtractor, a careful NLP tool that extracts clean, verifiable claims from messy text to combat misinformation. 

## Task
From the given INPUT TEXT, extract the **atomic factual claim** that is suitable for fact checking. This will be a single cleaned claim, normalized for verifiability and proper attribution.

### What counts as a "claim"?
- A claim must be an **assertion** or **fact** that can be verified as true/false or falsifiable (e.g., numerical, comparative, causational).
- You must **exclude** opinions, perspectives, or rhetorical questions unless they contain checkable predicates.
- Claims should be **context-independent** (e.g., causal statements, numerical data, temporal assertions).

### Output Format
Return a **strict JSON** that matches the SCHEMA below. Do not add commentary.

### SCHEMA
{
  "doc_meta": {
    "language": "en",
    "source_type": "string",  // provided as input
    "extraction_quality_note": "string"  // notes on parsing quality (if any)
  },
  "claims": [
    {
      "id": "C1",
      "text_span": "string",  // exact text of the claim
      "normalized": "string",  // cleaned/normalized claim text
      "type": "string",  // one of: existence, quantity, comparison, causation, prediction, correlation, attribution, definition, policy
      "topic": "string",  // general topic category (e.g., health, politics, tech)
      "subject_entities": [ {"name":"string","type":"string"} ],  // person/org/place/product/event/other
      "objects_entities":  [ {"name":"string","type":"string"} ],  // entity involved in the claim
      "temporal": {
        "when_text": "string|null",   // original temporal reference (e.g., "in 2020")
        "when_iso": "string|null"     // ISO format date (YYYY-MM-DD)
      },
      "location": "string|null",  // e.g., "USA", "New York"
      "quantity": {
        "value_text": "string|null",  // numerical value (e.g., "40%", "1000 users")
        "value_num": "number|null",
        "unit": "string|null"  // unit (e.g., "USD", "miles", "hours")
      },
      "stance": "string",  // one of: asserts, denies, questions, uncertain
      "modality_hedges": ["string"],  // e.g., "may", "could", "reportedly"
      "evidence_cues": {
        "urls": ["string"],  // any provided URLs
        "quoted_sources": ["string"],  // e.g., "CDC", "@elonmusk"
        "media_mentions": ["string"],  // images/videos mentioned
        "numbers_in_text": ["string"]  // any numbers explicitly mentioned
      },
      "sensitivity": {
        "domain": ["string"],  // e.g., "medical", "political"
        "harm_risk": "low|medium|high"  // risk triage (conservative in health/safety)
      },
      "verifiability": {
        "is_checkable": true,  // whether the claim is checkable
        "best_evidence_types": ["string"]  // types of evidence (e.g., RCT, news report)
      },
      "attribution": {
        "speaker": "string|null",  // who made the claim
        "speaker_type": "string|null"  // e.g., "expert", "journalist", "influencer"
      },
      "context": {
        "surrounding_sentence": "string|null",  // surrounding sentence context
        "thread_relation": "original|reply|quote|reshare|unknown"
      }
    }
  ],
  "non_claim_spans": ["string"]  // parts of the input that were ignored (non-checkable)
}

## Rules
- Ensure that the extracted claim is **atomic** and properly normalized for checkability.
- Normalize temporal expressions (e.g., convert "next week" to "YYYY-MM-DD").
- If the input is noisy (OCR/ASR errors), make reasonable guesses but mark uncertain claims with **stance="uncertain"**.
- Keep **modality_hedges** (like "may" or "could") in the output if the claim is uncertain.
- Only return **one cleaned claim** in the output (`claims[0]`).

"""

def call_openai_to_structure(text):
        system_msg = (
                "You are a strict JSON formatter. Convert user text into the JSON schema provided. "
                "Do not add any extra fields or commentary."
        )
        user_msg = f"{SCHEMA_INSTRUCTIONS}\n\nUser input:\n\"\"\"\n{text}\n\"\"\""
        try:
                resp = openai.responses.create(
                        model=MODEL_NAME,
                        input=[
                                {"role": "system", "content": system_msg},
                                {"role": "user", "content": user_msg},
                        ],
                        temperature=0.0
                )
        except Exception as e:
                raise RuntimeError(f"OpenAI API error: {e}")

        return resp.output_text

def extract_json_from_text(text):
        # Try direct parse first
        try:
                return json.loads(text)
        except Exception:
                # attempt to extract first {...} block
                m = re.search(r"(\{(?:.|\n)*\})", text)
                if m:
                        try:
                                return json.loads(m.group(1))
                        except Exception:
                                pass
        return None

def main():
        load_dotenv(ENV_PATH)
        while True:
            user_input = input("User: ")
            if user_input.lower() in ['exit', 'quit']:
                print("Exiting the chat.")
                break
            response_text = call_openai_to_structure(user_input)
            structured = extract_json_from_text(response_text)

            if structured is None:
                    print("Failed to parse JSON from model response. Raw response below:\n", file=sys.stderr)
                    print(response_text, file=sys.stderr)
                    sys.exit(2)

            # ensure original_input is present
            structured.setdefault("original_input", user_input)
            # pretty-print to stdout
            print(json.dumps(structured, indent=2, ensure_ascii=False))

if __name__ == "__main__":
        main()